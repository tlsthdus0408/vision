{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5404350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# The CIFAR-10 dataset:\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "\n",
    "def __unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def read_cifar_10(image_width, image_height):\n",
    "\n",
    "    batch_1 = __unpickle('./cifar-10/data_batch_1')\n",
    "    batch_2 = __unpickle('./cifar-10/data_batch_2')\n",
    "    batch_3 = __unpickle('./cifar-10/data_batch_3')\n",
    "    batch_4 = __unpickle('./cifar-10/data_batch_4')\n",
    "    batch_5 = __unpickle('./cifar-10/data_batch_5')\n",
    "    test_batch = __unpickle('./cifar-10/test_batch')\n",
    "\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    total_train_samples = len(batch_1[b'labels']) + len(batch_2[b'labels']) + len(batch_3[b'labels'])\\\n",
    "                          + len(batch_4[b'labels']) + len(batch_5[b'labels'])\n",
    "\n",
    "    X_train = np.zeros(shape=[total_train_samples, image_width, image_height, 3], dtype=np.uint8)\n",
    "    Y_train = np.zeros(shape=[total_train_samples, len(classes)], dtype=np.float32)\n",
    "\n",
    "    batches = [batch_1, batch_2, batch_3, batch_4, batch_5]\n",
    "\n",
    "    index = 0\n",
    "    for batch in batches:\n",
    "        for i in range(len(batch[b'labels'])):\n",
    "            image = batch[b'data'][i].reshape(3, 32, 32).transpose([1, 2, 0])\n",
    "            label = batch[b'labels'][i]\n",
    "\n",
    "            X = scipy.misc.imresize(image, size=(image_height, image_width), interp='bicubic')\n",
    "            Y = np.zeros(shape=[len(classes)], dtype=np.int)\n",
    "            Y[label] = 1\n",
    "\n",
    "            X_train[index + i] = X\n",
    "            Y_train[index + i] = Y\n",
    "\n",
    "        index += len(batch[b'labels'])\n",
    "\n",
    "    total_test_samples = len(test_batch[b'labels'])\n",
    "\n",
    "    X_test = np.zeros(shape=[total_test_samples, image_width, image_height, 3], dtype=np.uint8)\n",
    "    Y_test = np.zeros(shape=[total_test_samples, len(classes)], dtype=np.float32)\n",
    "\n",
    "    for i in range(len(test_batch[b'labels'])):\n",
    "        image = test_batch[b'data'][i].reshape(3, 32, 32).transpose([1, 2, 0])\n",
    "        label = test_batch[b'labels'][i]\n",
    "\n",
    "        X = scipy.misc.imresize(image, size=(image_height, image_width), interp='bicubic')\n",
    "        Y = np.zeros(shape=[len(classes)], dtype=np.int)\n",
    "        Y[label] = 1\n",
    "\n",
    "        X_test[i] = X\n",
    "        Y_test[i] = Y\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edfb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ZFNet:\n",
    "\n",
    "    def __init__(self, input_width=224, input_height=224, input_channels=3, num_classes=1000, learning_rate=0.01,\n",
    "                 momentum=0.9, keep_prob=0.5):\n",
    "\n",
    "        # From article: Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters,\n",
    "        # starting with a learning rate of 10**−2, in conjunction with a momentum term of 0.9.\n",
    "        # From article: Dropout is used in the fully connected layers (6 and 7) with a rate of 0.5.\n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        self.random_mean = 0\n",
    "        self.random_stddev = 0.01\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Input: 224x224x3.\n",
    "        with tf.name_scope('input'):\n",
    "            self.X = tf.placeholder(dtype=tf.float32,\n",
    "                                    shape=[None, self.input_height, self.input_width, self.input_channels], name='X')\n",
    "\n",
    "        # Labels: 1000.\n",
    "        with tf.name_scope('labels'):\n",
    "            self.Y = tf.placeholder(dtype=tf.float32, shape=[None, self.num_classes], name='Y')\n",
    "\n",
    "        # Dropout keep prob.\n",
    "        with tf.name_scope('dropout'):\n",
    "            self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='dropout_keep_prob')\n",
    "\n",
    "        # Layer 1.\n",
    "        # [Input] ==> 224x224x3\n",
    "        # --> 224x224x3 ==> [Convolution: size=(7x7x3)x96, strides=2, padding=valid] ==> 110x110x96\n",
    "        # --> 110x110x96 ==> [ReLU] ==> 110x110x96\n",
    "        # --> 110x110x96 ==> [Max-Pool: size=3x3, strides=2, padding=valid] ==> 55x55x96\n",
    "        # --> [Output] ==> 55x55x96\n",
    "        # Note: There were some calculation errors in ZFNet architecture:\n",
    "        # floor((224-7)/2) + 1 = 109\n",
    "        # floor((110-3)/2) + 1 = 54\n",
    "        with tf.name_scope('layer1'):\n",
    "            layer1_activations = self.__conv(input=self.X, filter_width=7, filter_height=7, filters_count=96,\n",
    "                                             stride_x=2, stride_y=2, padding='VALID')\n",
    "            layer1_pool = self.__max_pool(input=layer1_activations, filter_width=3, filter_height=3, stride_x=2,\n",
    "                                          stride_y=2, padding='VALID')\n",
    "\n",
    "        # Layer 2.\n",
    "        # [Input] ==> 55x55x96\n",
    "        # --> 55x55x96 ==> [Convolution: size=(5x5x96)x256, strides=2, padding=valid] ==> 26x26x256\n",
    "        # --> 26x26x256 ==> [ReLU] ==> 26x26x256\n",
    "        # --> 26x26x256 ==> [Max-Pool: size=3x3, strides=2, padding=valid] ==> 13x13x256\n",
    "        # --> [Output] ==> 13x13x256\n",
    "        # Note: There were some calculation errors in ZFNet architecture:\n",
    "        # floor((26-3)/2) + 1 = 12\n",
    "        with tf.name_scope('layer2'):\n",
    "            layer2_activations = self.__conv(input=layer1_pool, filter_width=5, filter_height=5, filters_count=256,\n",
    "                                             stride_x=2, stride_y=2, padding='VALID')\n",
    "            layer2_pool = self.__max_pool(input=layer2_activations, filter_width=3, filter_height=3, stride_x=2,\n",
    "                                          stride_y=2, padding='VALID')\n",
    "\n",
    "        # Layer 3.\n",
    "        # [Input] ==> 13x13x256\n",
    "        # --> 13x13x256 ==> [Convolution: size=(3x3x256)x384, strides=1, padding=same] ==> 13x13x384\n",
    "        # --> 13x13x384 ==> [ReLU] ==> 13x13x384\n",
    "        # --> [Output] ==> 13x13x384\n",
    "        with tf.name_scope('layer3'):\n",
    "            layer3_activations = self.__conv(input=layer2_pool, filter_width=3, filter_height=3, filters_count=384,\n",
    "                                             stride_x=1, stride_y=1, padding='SAME')\n",
    "\n",
    "        # Layer 4.\n",
    "        # [Input] ==> 13x13x384\n",
    "        # --> 13x13x384 ==> [Convolution: size=(3x3x384)x384, strides=1, padding=same] ==> 13x13x384\n",
    "        # --> 13x13x384 ==> [ReLU] ==> 13x13x384\n",
    "        # --> [Output] ==> 13x13x384\n",
    "        with tf.name_scope('layer4'):\n",
    "            layer4_activations = self.__conv(input=layer3_activations, filter_width=3, filter_height=3,\n",
    "                                             filters_count=384, stride_x=1, stride_y=1, padding='SAME')\n",
    "\n",
    "        # Layer 5.\n",
    "        # [Input] ==> 13x13x384\n",
    "        # --> 13x13x384 ==> [Convolution: size=(3x3x384)x256, strides=1, padding=same] ==> 13x13x256\n",
    "        # --> 13x13x256 ==> [ReLU] ==> 13x13x256\n",
    "        # --> 13x13x256 ==> [Max-Pool: size=3x3, strides=2, padding=valid] ==> 6x6x256\n",
    "        # --> [Output] ==> 6x6x256\n",
    "        with tf.name_scope('layer5'):\n",
    "            layer5_activations = self.__conv(input=layer4_activations, filter_width=3, filter_height=3,\n",
    "                                             filters_count=256, stride_x=1, stride_y=1, padding='SAME')\n",
    "            layer5_pool = self.__max_pool(input=layer5_activations, filter_width=3, filter_height=3, stride_x=2,\n",
    "                                          stride_y=2, padding='VALID')\n",
    "\n",
    "        # Layer 6.\n",
    "        # [Input] ==> 6x6x256=9216\n",
    "        # --> 9216 ==> [Fully Connected: neurons=4096] ==> 4096\n",
    "        # --> 4096 ==> [ReLU] ==> 4096\n",
    "        # --> 4096 ==> [Dropout] ==> 4096\n",
    "        # --> [Output] ==> 4096\n",
    "        with tf.name_scope('layer6'):\n",
    "            pool5_shape = layer5_pool.get_shape().as_list()\n",
    "            flattened_input_size = pool5_shape[1] * pool5_shape[2] * pool5_shape[3]\n",
    "            layer6_fc = self.__fully_connected(input=tf.reshape(layer5_pool, shape=[-1, flattened_input_size]),\n",
    "                                               inputs_count=flattened_input_size, outputs_count=4096, relu=True)\n",
    "            layer6_dropout = self.__dropout(input=layer6_fc)\n",
    "\n",
    "        # Layer 7.\n",
    "        # [Input] ==> 4096\n",
    "        # --> 4096 ==> [Fully Connected: neurons=4096] ==> 4096\n",
    "        # --> 4096 ==> [ReLU] ==> 4096\n",
    "        # --> 4096 ==> [Dropout] ==> 4096\n",
    "        # --> [Output] ==> 4096\n",
    "        with tf.name_scope('layer7'):\n",
    "            layer7_fc = self.__fully_connected(input=layer6_dropout, inputs_count=4096, outputs_count=4096, relu=True)\n",
    "            layer7_dropout = self.__dropout(input=layer7_fc)\n",
    "\n",
    "        # Layer 8.\n",
    "        # [Input] ==> 4096\n",
    "        # --> 4096 ==> [Logits: neurons=1000] ==> 1000\n",
    "        # --> [Output] ==> 1000\n",
    "        with tf.name_scope('layer8'):\n",
    "            layer8_logits = self.__fully_connected(input=layer7_dropout, inputs_count=4096,\n",
    "                                                   outputs_count=self.num_classes, relu=False, name='logits')\n",
    "\n",
    "        # Cross Entropy.\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer8_logits, labels=self.Y,\n",
    "                                                                       name='cross_entropy')\n",
    "            self.__variable_summaries(cross_entropy)\n",
    "\n",
    "        # Training.\n",
    "        with tf.name_scope('training'):\n",
    "            loss_operation = tf.reduce_mean(cross_entropy, name='loss_operation')\n",
    "            tf.summary.scalar(name='loss', tensor=loss_operation)\n",
    "\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "\n",
    "            # self.training_operation = optimizer.minimize(loss_operation, name='training_operation')\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(loss_operation)\n",
    "            self.training_operation = optimizer.apply_gradients(grads_and_vars, name='training_operation')\n",
    "\n",
    "            for grad, var in grads_and_vars:\n",
    "                if grad is not None:\n",
    "                    with tf.name_scope(var.op.name + '/gradients'):\n",
    "                        self.__variable_summaries(grad)\n",
    "\n",
    "        # Accuracy.\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(layer8_logits, 1), tf.argmax(self.Y, 1), name='correct_prediction')\n",
    "            self.accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy_operation')\n",
    "            tf.summary.scalar(name='accuracy', tensor=self.accuracy_operation)\n",
    "\n",
    "    def train_epoch(self, sess, X_data, Y_data, batch_size=128, file_writer=None, summary_operation=None,\n",
    "                    epoch_number=None):\n",
    "        num_examples = len(X_data)\n",
    "        step = 0\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_data[offset:end], Y_data[offset:end]\n",
    "            if file_writer is not None and summary_operation is not None:\n",
    "                _, summary = sess.run([self.training_operation, summary_operation],\n",
    "                                      feed_dict={self.X: batch_x, self.Y: batch_y,\n",
    "                                                 self.dropout_keep_prob: self.keep_prob})\n",
    "                file_writer.add_summary(summary, epoch_number * (num_examples // batch_size + 1) + step)\n",
    "                step += 1\n",
    "            else:\n",
    "                sess.run(self.training_operation, feed_dict={self.X: batch_x, self.Y: batch_y,\n",
    "                                                             self.dropout_keep_prob: self.keep_prob})\n",
    "\n",
    "    def evaluate(self, sess, X_data, Y_data, batch_size=128):\n",
    "        num_examples = len(X_data)\n",
    "        total_accuracy = 0\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_x, batch_y = X_data[offset:end], Y_data[offset:end]\n",
    "            batch_accuracy = sess.run(self.accuracy_operation, feed_dict={self.X: batch_x, self.Y: batch_y,\n",
    "                                                                          self.dropout_keep_prob: 1.0})\n",
    "            total_accuracy += (batch_accuracy * len(batch_x))\n",
    "        return total_accuracy / num_examples\n",
    "\n",
    "    def save(self, sess, file_name):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, file_name)\n",
    "\n",
    "    def restore(self, sess, checkpoint_dir):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    def __variable_summaries(self, var):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def __initial_weight_values(self, shape):\n",
    "        # return tf.constant(value=0.01, dtype=tf.float32, shape=shape)\n",
    "        return tf.random_normal(shape=shape, mean=self.random_mean, stddev=self.random_stddev, dtype=tf.float32)\n",
    "\n",
    "    def __conv(self, input, filter_width, filter_height, filters_count, stride_x, stride_y, padding='VALID',\n",
    "               name='conv'):\n",
    "        # From article: All weights are initialized to 10**−2 and biases are set to 0.\n",
    "        with tf.name_scope(name):\n",
    "            input_channels = input.get_shape()[-1].value\n",
    "            filters = tf.Variable(\n",
    "                self.__initial_weight_values(shape=[filter_height, filter_width, input_channels, filters_count]),\n",
    "                name='filters')\n",
    "            convs = tf.nn.conv2d(input=input, filter=filters, strides=[1, stride_y, stride_x, 1], padding=padding,\n",
    "                                 name='convs')\n",
    "            biases = tf.Variable(tf.zeros(shape=[filters_count], dtype=tf.float32), name='biases')\n",
    "            preactivations = tf.nn.bias_add(convs, biases, name='preactivations')\n",
    "            activations = tf.nn.relu(preactivations, name='activations')\n",
    "\n",
    "            with tf.name_scope('filter_summaries'):\n",
    "                self.__variable_summaries(filters)\n",
    "\n",
    "            with tf.name_scope('bias_summaries'):\n",
    "                self.__variable_summaries(biases)\n",
    "\n",
    "            with tf.name_scope('preactivations_histogram'):\n",
    "                tf.summary.histogram('preactivations', preactivations)\n",
    "\n",
    "            with tf.name_scope('activations_histogram'):\n",
    "                tf.summary.histogram('activations', activations)\n",
    "\n",
    "            return activations\n",
    "\n",
    "    def __max_pool(self, input, filter_width, filter_height, stride_x, stride_y, padding='VALID', name='pool'):\n",
    "        with tf.name_scope(name):\n",
    "            pool = tf.nn.max_pool(input, ksize=[1, filter_height, filter_width, 1], strides=[1, stride_y, stride_x, 1],\n",
    "                                  padding=padding, name='pool')\n",
    "            return pool\n",
    "\n",
    "    def __fully_connected(self, input, inputs_count, outputs_count, relu=True, name='fully_connected'):\n",
    "        with tf.name_scope(name):\n",
    "            wights = tf.Variable(self.__initial_weight_values(shape=[inputs_count, outputs_count]), name='wights')\n",
    "            biases = tf.Variable(tf.zeros(shape=[outputs_count], dtype=tf.float32), name='biases')\n",
    "            preactivations = tf.nn.bias_add(tf.matmul(input, wights), biases, name='preactivations')\n",
    "            if relu:\n",
    "                activations = tf.nn.relu(preactivations, name='activations')\n",
    "\n",
    "            with tf.name_scope('wight_summaries'):\n",
    "                self.__variable_summaries(wights)\n",
    "\n",
    "            with tf.name_scope('bias_summaries'):\n",
    "                self.__variable_summaries(biases)\n",
    "\n",
    "            with tf.name_scope('preactivations_histogram'):\n",
    "                tf.summary.histogram('preactivations', preactivations)\n",
    "\n",
    "            if relu:\n",
    "                with tf.name_scope('activations_histogram'):\n",
    "                    tf.summary.histogram('activations', activations)\n",
    "\n",
    "            if relu:\n",
    "                return activations\n",
    "            else:\n",
    "                return preactivations\n",
    "\n",
    "    def __dropout(self, input, name='dropout'):\n",
    "        with tf.name_scope(name):\n",
    "            return tf.nn.dropout(input, keep_prob=self.dropout_keep_prob, name='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9329ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CIFAR-10...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cifar-10/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading CIFAR-10...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m X_train, Y_train, X_test, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mread_cifar_10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_height\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_HEIGHT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m zfnet \u001b[38;5;241m=\u001b[39m ZFNet(input_width\u001b[38;5;241m=\u001b[39mINPUT_WIDTH, input_height\u001b[38;5;241m=\u001b[39mINPUT_HEIGHT, input_channels\u001b[38;5;241m=\u001b[39mINPUT_CHANNELS,\n\u001b[0;32m     19\u001b[0m               num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES, learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE, momentum\u001b[38;5;241m=\u001b[39mMOMENTUM, keep_prob\u001b[38;5;241m=\u001b[39mKEEP_PROB)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m sess:\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mread_cifar_10\u001b[1;34m(image_width, image_height)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_cifar_10\u001b[39m(image_width, image_height):\n\u001b[1;32m---> 19\u001b[0m     batch_1 \u001b[38;5;241m=\u001b[39m \u001b[43m__unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./cifar-10/data_batch_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     batch_2 \u001b[38;5;241m=\u001b[39m __unpickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cifar-10/data_batch_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m     batch_3 \u001b[38;5;241m=\u001b[39m __unpickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cifar-10/data_batch_3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m__unpickle\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__unpickle\u001b[39m(file):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fo, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cifar-10/data_batch_1'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec792eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
